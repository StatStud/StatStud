{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq8nuP0YXSo/uK74JZAOww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55e4c845fcec4066a3d361604cb2fa69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e16d5ae2dd58439cade6f7165212a44c",
              "IPY_MODEL_b3ec09df91454003911fb4adfee89011",
              "IPY_MODEL_b7364b4d63604c3fa240c36039b8f921"
            ],
            "layout": "IPY_MODEL_6c13eab24815485b8ce948aea42d5f1f"
          }
        },
        "e16d5ae2dd58439cade6f7165212a44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fac9bc916dc4b74ae165d51d2b6e5b2",
            "placeholder": "​",
            "style": "IPY_MODEL_a8909628e940449ebd785a95b28d263b",
            "value": "100%"
          }
        },
        "b3ec09df91454003911fb4adfee89011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ab1da5bf2534fc7b6bfdcfa6a5097f5",
            "max": 17022,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5322f32f9dc4f89a4a710e7b8e3027f",
            "value": 17022
          }
        },
        "b7364b4d63604c3fa240c36039b8f921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d77eb4c3dd90473a844be666ae28df16",
            "placeholder": "​",
            "style": "IPY_MODEL_7053fd65085542bdba021d9c1342ed3a",
            "value": " 17022/17022 [02:47&lt;00:00, 119.74ex/s]"
          }
        },
        "6c13eab24815485b8ce948aea42d5f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fac9bc916dc4b74ae165d51d2b6e5b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8909628e940449ebd785a95b28d263b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ab1da5bf2534fc7b6bfdcfa6a5097f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5322f32f9dc4f89a4a710e7b8e3027f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d77eb4c3dd90473a844be666ae28df16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7053fd65085542bdba021d9c1342ed3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StatStud/StatStud/blob/main/Entity_Linking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68PISvXyIzOQ",
        "outputId": "36c8027e-cd0a-48e3-bfc8-b25f63e57f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp==2.1.0\n",
            "  Downloading allennlp-2.1.0-py3-none-any.whl (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.1/585.1 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting allennlp-models==2.1.0\n",
            "  Downloading allennlp_models-2.1.0-py3-none-any.whl (407 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.9/407.9 KB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock<3.1,>=3.0\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting jsonpickle\n",
            "  Downloading jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.26.99-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.6/593.6 KB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch<1.8.0,>=1.6.0\n",
            "  Downloading torch-1.7.1-cp39-cp39-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy<3.1,>=2.1.0\n",
            "  Downloading spacy-3.0.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.19\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<4.4,>=4.1\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.18\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.9.0,>=0.8.1\n",
            "  Downloading torchvision-0.8.2-cp39-cp39-manylinux1_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lmdb\n",
            "  Downloading lmdb-1.4.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.9/305.9 KB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytest\n",
            "  Downloading pytest-7.2.2-py3-none-any.whl (317 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 KB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting more-itertools\n",
            "  Downloading more_itertools-9.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu==4.4\n",
            "  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.99\n",
            "  Downloading botocore-1.29.99-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp39-cp39-manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
            "Collecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.6/491.6 KB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.5\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting blis<0.8.0,>=0.4.0\n",
            "  Downloading blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-67.6.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "  Downloading thinc-8.0.17-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (668 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.8/668.8 KB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting protobuf<4,>=3.8.0\n",
            "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting pillow>=4.1.1\n",
            "  Downloading Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 KB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wcwidth>=0.2.5\n",
            "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting attrs>=19.2.0\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iniconfig\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
            "Collecting tomli>=1.0.0\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting python-dateutil<3.0.0,>=2.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: overrides, jsonnet, word2number, sacremoses\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10189 sha256=9084dd6d6ac696d49083d0637f8cc789b2005a0dd19a1842e05002abe6704275\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/11/0e/73fdcb3d71d97e33c230900efe85923ee9d49515d050503174\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp39-cp39-linux_x86_64.whl size=6333864 sha256=412ed34805a3c4d38f491047d065a9021d6be2b4ae64e1816a764c3209b21d95\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/38/9b/973b06cba1df6acc551937b244246969a31f5ef2820331c056\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=df59fca6321758f08c90ca3a831d7fb96ec91cb4fb3850dbc960fc1d2a9355bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/4a/5b/d2f2df5c344ddbecb8bea759872c207ea91d93f57fb54e816e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=f291bf673cd321b8e9c55d2af483b3976a3c7897c00fe723f9e08ab3f140dd43\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built overrides jsonnet word2number sacremoses\n",
            "Installing collected packages: word2number, wcwidth, wasabi, tokenizers, sentencepiece, py-rouge, overrides, lmdb, jsonnet, filelock, cymem, conllu, urllib3, typing-extensions, tqdm, tomli, threadpoolctl, spacy-legacy, smart-open, six, setuptools, regex, protobuf, pluggy, pillow, packaging, numpy, murmurhash, more-itertools, MarkupSafe, jsonpickle, joblib, jmespath, iniconfig, idna, ftfy, exceptiongroup, click, charset-normalizer, certifi, catalogue, attrs, typer, torch, tensorboardX, srsly, scipy, sacremoses, requests, python-dateutil, pytest, pydantic, preshed, nltk, jinja2, h5py, blis, transformers, torchvision, thinc, scikit-learn, pathy, botocore, spacy, s3transfer, boto3, allennlp, allennlp-models\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.6\n",
            "    Uninstalling wcwidth-0.2.6:\n",
            "      Successfully uninstalled wcwidth-0.2.6\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.10.1\n",
            "    Uninstalling filelock-3.10.1:\n",
            "      Successfully uninstalled filelock-3.10.1\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.7\n",
            "    Uninstalling cymem-2.0.7:\n",
            "      Successfully uninstalled cymem-2.0.7\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: tomli\n",
            "    Found existing installation: tomli 2.0.1\n",
            "    Uninstalling tomli-2.0.1:\n",
            "      Successfully uninstalled tomli-2.0.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: spacy-legacy\n",
            "    Found existing installation: spacy-legacy 3.0.12\n",
            "    Uninstalling spacy-legacy-3.0.12:\n",
            "      Successfully uninstalled spacy-legacy-3.0.12\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.3.0\n",
            "    Uninstalling smart-open-6.3.0:\n",
            "      Successfully uninstalled smart-open-6.3.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.6.0\n",
            "    Uninstalling setuptools-67.6.0:\n",
            "      Successfully uninstalled setuptools-67.6.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.10.31\n",
            "    Uninstalling regex-2022.10.31:\n",
            "      Successfully uninstalled regex-2022.10.31\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 1.0.0\n",
            "    Uninstalling pluggy-1.0.0:\n",
            "      Successfully uninstalled pluggy-1.0.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.9\n",
            "    Uninstalling murmurhash-1.0.9:\n",
            "      Successfully uninstalled murmurhash-1.0.9\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 9.1.0\n",
            "    Uninstalling more-itertools-9.1.0:\n",
            "      Successfully uninstalled more-itertools-9.1.0\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.2\n",
            "    Uninstalling MarkupSafe-2.1.2:\n",
            "      Successfully uninstalled MarkupSafe-2.1.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.1\n",
            "    Uninstalling joblib-1.1.1:\n",
            "      Successfully uninstalled joblib-1.1.1\n",
            "  Attempting uninstall: iniconfig\n",
            "    Found existing installation: iniconfig 2.0.0\n",
            "    Uninstalling iniconfig-2.0.0:\n",
            "      Successfully uninstalled iniconfig-2.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.1\n",
            "    Uninstalling exceptiongroup-1.1.1:\n",
            "      Successfully uninstalled exceptiongroup-1.1.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.12.7\n",
            "    Uninstalling certifi-2022.12.7:\n",
            "      Successfully uninstalled certifi-2022.12.7\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 22.2.0\n",
            "    Uninstalling attrs-22.2.0:\n",
            "      Successfully uninstalled attrs-22.2.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.6\n",
            "    Uninstalling srsly-2.4.6:\n",
            "      Successfully uninstalled srsly-2.4.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.2.2\n",
            "    Uninstalling pytest-7.2.2:\n",
            "      Successfully uninstalled pytest-7.2.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.7\n",
            "    Uninstalling pydantic-1.10.7:\n",
            "      Successfully uninstalled pydantic-1.10.7\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.8\n",
            "    Uninstalling preshed-3.0.8:\n",
            "      Successfully uninstalled preshed-3.0.8\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.2\n",
            "    Uninstalling Jinja2-3.1.2:\n",
            "      Successfully uninstalled Jinja2-3.1.2\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.8.0\n",
            "    Uninstalling h5py-3.8.0:\n",
            "      Successfully uninstalled h5py-3.8.0\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.9\n",
            "    Uninstalling thinc-8.1.9:\n",
            "      Successfully uninstalled thinc-8.1.9\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pathy\n",
            "    Found existing installation: pathy 0.10.1\n",
            "    Uninstalling pathy-0.10.1:\n",
            "      Successfully uninstalled pathy-0.10.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.1\n",
            "    Uninstalling spacy-3.5.1:\n",
            "      Successfully uninstalled spacy-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.2.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\n",
            "inflect 6.0.2 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "flask 2.2.3 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.0.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 allennlp-2.1.0 allennlp-models-2.1.0 attrs-22.2.0 blis-0.7.9 boto3-1.26.99 botocore-1.29.99 catalogue-2.0.8 certifi-2022.12.7 charset-normalizer-3.1.0 click-7.1.2 conllu-4.4 cymem-2.0.7 exceptiongroup-1.1.1 filelock-3.0.12 ftfy-6.1.1 h5py-3.8.0 idna-3.4 iniconfig-2.0.0 jinja2-3.1.2 jmespath-1.0.1 joblib-1.2.0 jsonnet-0.19.1 jsonpickle-3.0.1 lmdb-1.4.0 more-itertools-9.1.0 murmurhash-1.0.9 nltk-3.8.1 numpy-1.24.2 overrides-3.1.0 packaging-23.0 pathy-0.10.1 pillow-9.4.0 pluggy-1.0.0 preshed-3.0.8 protobuf-3.20.3 py-rouge-1.1 pydantic-1.8.2 pytest-7.2.2 python-dateutil-2.8.2 regex-2023.3.23 requests-2.28.2 s3transfer-0.6.0 sacremoses-0.0.53 scikit-learn-1.2.2 scipy-1.10.1 sentencepiece-0.1.97 setuptools-67.6.0 six-1.16.0 smart-open-6.3.0 spacy-3.0.9 spacy-legacy-3.0.12 srsly-2.4.6 tensorboardX-2.6 thinc-8.0.17 threadpoolctl-3.1.0 tokenizers-0.10.3 tomli-2.0.1 torch-1.7.1 torchvision-0.8.2 tqdm-4.65.0 transformers-4.3.3 typer-0.3.2 typing-extensions-4.5.0 urllib3-1.26.15 wasabi-0.10.1 wcwidth-0.2.6 word2number-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --force-reinstall allennlp==2.1.0 allennlp-models==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging==21.3 datasets==2.7.1 scispacy==0.5.1 spacy==3.4.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oqXq-ikOuvX",
        "outputId": "5f406a23-064c-40ef-f8e8-87dd1026b9e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting packaging==21.3\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.7.1\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scispacy==0.5.1\n",
            "  Downloading scispacy-0.5.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==3.4.3\n",
            "  Downloading spacy-3.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging==21.3) (3.0.9)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.7.1) (2.28.2)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.7.1) (1.4.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.7.1) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.7.1) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.7.1) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.7.1) (2023.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.7.1) (1.24.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from scispacy==0.5.1) (1.2.0)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.9/dist-packages (from scispacy==0.5.1) (4.4)\n",
            "Collecting nmslib>=1.7.3.6\n",
            "  Downloading nmslib-2.1.1-cp39-cp39-manylinux2010_x86_64.whl (13.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.9/dist-packages (from scispacy==0.5.1) (1.2.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (1.0.4)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Downloading thinc-8.1.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.5/922.5 KB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (67.6.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.0.12)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (0.3.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (2.4.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.7.1) (3.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.7.1) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.7.1) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.7.1) (4.5.0)\n",
            "Collecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from nmslib>=1.7.3.6->scispacy==0.5.1) (5.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy>=0.3.5->spacy==3.4.3) (6.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.7.1) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.7.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.7.1) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.3->scispacy==0.5.1) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.3->scispacy==0.5.1) (1.10.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.3) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.3) (0.7.9)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy==3.4.3) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy==3.4.3) (2.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.7.1) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.7.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.7.1) (1.16.0)\n",
            "Installing collected packages: xxhash, pysbd, pybind11, packaging, multidict, frozenlist, dill, async-timeout, yarl, responses, nmslib, multiprocess, huggingface-hub, aiosignal, thinc, aiohttp, spacy, scispacy, datasets\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.0.9\n",
            "    Uninstalling spacy-3.0.9:\n",
            "      Successfully uninstalled spacy-3.0.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.4.3 which is incompatible.\n",
            "allennlp 2.1.0 requires spacy<3.1,>=2.1.0, but you have spacy 3.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.7.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.3 multidict-6.0.4 multiprocess-0.70.14 nmslib-2.1.1 packaging-21.3 pybind11-2.6.1 pysbd-0.3.4 responses-0.18.0 scispacy-0.5.1 spacy-3.4.3 thinc-8.1.9 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.predictors.predictor import Predictor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "qctYw1cpPdxJ",
        "outputId": "0e32c8d7-0008-461b-a073-ccf7f6f5d361"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2901\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2902\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: _DistInfoDistribution__dep_map",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3095\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2901\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2902\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: _pkg_info",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0c2b06a8c759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/allennlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# On some systems this prevents the dreaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# ImportError: dlopen: cannot load any more object with static TLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .file_utils import (\n\u001b[1;32m     45\u001b[0m     \u001b[0m_BaseLazyModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\"\" require_version wrapper which emits a core-specific hint on failure \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# check if any version is installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mgot_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"this application\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \"\"\"\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m             \u001b[0mnew_requirements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m             \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_requirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   2819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0;34m\"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2821\u001b[0;31m         \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2822\u001b[0m         \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_compute_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3114\u001b[0m         \u001b[0mreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m         \u001b[0;31m# Including any condition expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parsed_pkg_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Requires-Dist'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m             \u001b[0mreqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3096\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3098\u001b[0;31m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPKG_INFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3099\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1515\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metadata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.9/dist-packages/joblib-1.1.1.dist-info/METADATA'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKEpk4ioQQuI",
        "outputId": "79db670b-9482-4bf9-8d22-eb059496a10a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "hoLrxEgVSgCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjqBmh7mQcTb",
        "outputId": "0e2b5fa3-c166-4ab3-e254-797567654280"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from jsonlines) (22.2.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz"
      ],
      "metadata": {
        "id": "hKvnDsGaTNmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "from collections import namedtuple \n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import time\n",
        "import torch\n",
        "from scipy.spatial.distance import cosine\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "import nltk\n",
        "import jsonlines\n",
        "import json\n",
        "import scispacy"
      ],
      "metadata": {
        "id": "hhfeu7uoPVCt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class entity_linker:\n",
        "    def __init__(self, embeddings_data=\"primekg_embeddings_synonym.pickle\",\n",
        "                 datafile = \"nodes_full.csv\", \n",
        "                 model_name = \"tekraj/avodamed-synonym-generator1\"):\n",
        "        self.embeddings_data = embeddings_data\n",
        "        self.datafile = datafile\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        self.srl_model_link = \"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\"\n",
        "        self.srl_model = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
        "        self.medspacy_model = spacy.load('en_core_web_sm')\n",
        "        \n",
        "\n",
        "    def run_pre_embedd(self):\n",
        "        \"\"\"\n",
        "        Generate Emneddings and FAISS indices for given nodes.\n",
        "        NOTE: Assuming column with node names is called \"node_name\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        self.datafile : data csv file\n",
        "            a csv file of the nodes. Column \"node_name\" must be present\n",
        "        Returns:\n",
        "        --------\n",
        "        NO EXPLICT RETURN VALUE\n",
        "        pickle file\n",
        "            Pickle file of Huggingface dataset insatance is saved\n",
        "        \"\"\"\n",
        "        print(\"starting pre-embedding\")\n",
        "        hugging_dataset = Dataset.from_pandas(pd.read_csv(self.datafile))    \n",
        "\n",
        "        column_to_embedd = \"node_name\"\n",
        "\n",
        "        embeddings_dataset = hugging_dataset.map(\n",
        "            lambda x: {\"embeddings\": self.get_embeddings(x[column_to_embedd]).detach().cpu().numpy()[0]}\n",
        "        )\n",
        "        print(\"embeddings done\")\n",
        "\n",
        "        embeddings_dataset.add_faiss_index(column='embeddings')\n",
        "        print(\"faiss added\")\n",
        "        \n",
        "        output_file_name = f\"{self.datafile[:-4]}_embeddings_{self.model_name.split('/')[-1] if '/' in self.model_name else self.model_name}.pickle\"\n",
        "\n",
        "        with open(output_file_name, \"wb\") as f:\n",
        "            pickle.dump(embeddings_dataset,f)\n",
        "\n",
        "        print(\"embeddings saved\")\n",
        "        \n",
        "    def get_embeddings(self,text_list):\n",
        "        \"\"\"\n",
        "        Sub function for self.run_pre_embedd().\n",
        "        Applies the actual embedding to dataset.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text_list : HuggingFace dataset column\n",
        "            The column from the huggingface dataset instance containing the node name\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        Embeddings\n",
        "            Final embeddings.\n",
        "        \"\"\"\n",
        "        encoded_input = self.tokenizer(\n",
        "            text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        encoded_input = {k: v for k, v in encoded_input.items()}\n",
        "        #move the encoded input to the device\n",
        "        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
        "        model_output = self.model(**encoded_input)\n",
        "        return self.cls_pooling(model_output)\n",
        "\n",
        "    def cls_pooling(self,model_output):\n",
        "        \"\"\"\n",
        "        Sub function of self.get_embeddings.\n",
        "        This function takes in model_output as input, which is the output obtained after \n",
        "        passing the input text to the pre-trained BERT model, and returns the first (CLS) \n",
        "        token from the last hidden state of the model output.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model_output : pytorch tensor\n",
        "            A tensor representing the output of the pre-trained BERT model after passing the input text through it.\n",
        "        Returns:\n",
        "        --------\n",
        "        pytorch tensor\n",
        "            A tensor representing the first (CLS) token from the last hidden state of the pre-trained BERT model output.\n",
        "        \"\"\"\n",
        "        return model_output.last_hidden_state[:, 0]\n",
        "    \n",
        "    def extract_sentences(self,text):\n",
        "        \"\"\"\n",
        "        This function takes in a string of text as input and returns a list of sentences extracted from that text. \n",
        "        It uses the sent_tokenize function from the nltk library to tokenize the input text into sentences.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : String\n",
        "            The context/paragraph \n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            A list of sentences from the context/paragraph\n",
        "        \"\"\"\n",
        "        return nltk.sent_tokenize(text)\n",
        "    \n",
        "    def extract_arg1_words(self,data):\n",
        "        \"\"\"\n",
        "        Sub function of self.srl_extractor. This function takes in a \n",
        "        dictionary data as input, which contains the output of the \n",
        "        Semantic Role Labeling (SRL) model for a given sentence, and \n",
        "        returns a list of words that correspond to the ARG1 (agent) role in the sentence.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : dictionary\n",
        "            A dictionary containing the output of the SRL model for a given sentence.\n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            A list of strings representing the words that correspond to the ARG1 role in the sentence.\n",
        "        \"\"\"\n",
        "        arg1_words = []\n",
        "        for verb in data['verbs']:\n",
        "            description = verb['description']\n",
        "            arg1 = re.findall(r\"ARG1:\\s([\\w\\s]+)\", description)\n",
        "            if arg1:\n",
        "                arg1_words.append(arg1[0].strip())\n",
        "        return arg1_words    \n",
        "\n",
        "    def srl_extractor(self,sent):\n",
        "        \"\"\"\n",
        "        This function takes in a string of text as input, \n",
        "        uses the Semantic Role Labeling (SRL) model to extract \n",
        "        the ARG1 words from each sentence in the input text, and \n",
        "        returns a list of all the ARG1 words extracted from the text.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        sent : String\n",
        "            A string representing one sentence from the context\n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            A list of strings representing the ARG1 words extracted from the input text.\n",
        "        \"\"\"\n",
        "        pred = self.srl_model.predict(sentence=sent)\n",
        "        return self.extract_arg1_words(pred)\n",
        "    \n",
        "    def regex_entity_extract(self,sent):\n",
        "        \"\"\"\n",
        "        Sub function of self.named_entity_extractor.\n",
        "        Regex entity extractor, in case medspacy returns\n",
        "        empyt list.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        sent : String\n",
        "            A string representing one sentence from the context\n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            List of relevant string terms\n",
        "        \"\"\"\n",
        "        options = [\"lethargic\", \"shortness of breath\",\n",
        "                  \"headache\",\"hypertension\",\"chest palpitations\",\n",
        "                  \"chest pain\", \"shortness of breath\", \"throat\", \n",
        "                  \"inflamed\", \"interstitial fibrosis\", 'aspirin',\n",
        "                  \"reticulonodular infiltrate\",\"eggshell calcification\",\n",
        "                  \"adenopathies\", \"lisinopril\", \"metoprolol\",\"warfarin\",\n",
        "                  \"left atrium\", \"enlarged left ventricle\", \"nausea\",\n",
        "                  'diaphoretic', 'epigastric area','asthma',\n",
        "                  'diabetes mellitus', \"chronic bronchitis\"]\n",
        "        pattern = r\"\\b(\" + \"|\".join(map(re.escape, options)) + r\")\\b\"\n",
        "        matches = re.findall(pattern, sent)\n",
        "\n",
        "        if matches:\n",
        "            return matches\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "    def named_entity_extractor(self,sentence):\n",
        "        \"\"\"\n",
        "        MedSpacy entity extractor.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        sentence : String\n",
        "            A string representing one sentence from the context\n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            Description of the return value.\n",
        "        \"\"\"\n",
        "        docs = self.medspacy_model(sentence)\n",
        "        result = list(docs.ents)\n",
        "        result = [str(x) for x in result]\n",
        "\n",
        "        if result:\n",
        "            return result\n",
        "        else:\n",
        "            return self.regex_entity_extract(sentence)\n",
        "        \n",
        "    def merge_lists(self,lst1, lst2):\n",
        "        \"\"\"\n",
        "        Sub function for self.run_all.\n",
        "        Combines extracted queries from SRL and \n",
        "        NER and combines as a single list of queries \n",
        "        for each sentence in the context.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        lst : list[Str]\n",
        "            Either SRL list of NER list. Order does not matter\n",
        "        lst2 : list[Str]\n",
        "            Either SRL list of NER list. Order does not matter\n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            combined list of string queries, for each sentnece\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for sublst1, sublst2 in zip(lst1, lst2):\n",
        "            result.append(sublst1 + sublst2)\n",
        "        return result\n",
        "    \n",
        "    def run_query_faiss(self,query):\n",
        "        \"\"\"\n",
        "        Sub function for self.get_embedding_matches.\n",
        "        Obtain the top_k closest vector in vector space of nodes.\n",
        "        This assumes that the pre-embedding file is saved and previously\n",
        "        defined.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        query : String\n",
        "            Either the SRL or NER string element representing the search query\n",
        "        self.embeddings_data : pickle\n",
        "            The pre-saved embeddings data. See self.run_pre_embedd function\n",
        "        top_k: int\n",
        "            The count of candidates we wish to return.\n",
        "        Returns:\n",
        "        --------\n",
        "        pandas df\n",
        "            Pandas dataframe containing the top_k node node_names (with \n",
        "            corresponding scores) that are closest in vector space to query\n",
        "        \"\"\"\n",
        "        \n",
        "        top_k = 1\n",
        "\n",
        "        with open(self.embeddings_data, \"rb\") as f:\n",
        "            hugging_dataset = pickle.load(f)    \n",
        "\n",
        "        if (not isinstance(query, str)) or (not isinstance(query, list)):\n",
        "            query = str(query)\n",
        "\n",
        "        query_embedding = self.get_embeddings([query]).cpu().detach().numpy()\n",
        "\n",
        "        scores, samples = hugging_dataset.get_nearest_examples(\n",
        "            \"embeddings\", query_embedding, k=top_k\n",
        "        )\n",
        "\n",
        "        samples_df = pd.DataFrame.from_dict(samples)\n",
        "        samples_df[\"scores\"] = scores\n",
        "        samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n",
        "        return samples_df[['node_name']]\n",
        "    \n",
        "    def get_embedding_matches(self,query_lst):\n",
        "        \"\"\"\n",
        "        Returns consolidated FAISS matching results for\n",
        "        each context sentence.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        query_lst : list[Str]\n",
        "            List of extracted string queries from SRL and NER\n",
        "        Returns:\n",
        "        --------\n",
        "        List[Dict(Tuple)]\n",
        "            A list of dictionary values, where each dictionary contains the \n",
        "            top_k nearest node matches for each query of each context sentence.\n",
        "            The keys of the dictionary are the query for a given sentence, and the value\n",
        "            is a tuple, where the first element is the node_name of the closest match, and\n",
        "            the second element is the FAISS score (lower is better) of the match.\n",
        "        \"\"\"\n",
        "        result_lst = []\n",
        "        for lst in query_lst:\n",
        "            output_dict = {}\n",
        "            for query in lst:\n",
        "                query_results = self.run_query_faiss(query).values.tolist()\n",
        "                output_dict[query] = [(result[0]) for result in query_results][0]\n",
        "            result_lst.append(output_dict)\n",
        "        return result_lst\n",
        "    \n",
        "    def extract_top_strings(self,list_of_dicts):\n",
        "        \"\"\"\n",
        "        Applied after running self.get_embedding_matches().\n",
        "        Extracts the top 1 values (instead of top_k) final \n",
        "        node candidate for each sentence.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        list_of_dicts : List[Dict(Tuple)]\n",
        "            Consolidated list of FAISS results. See output for self.get_embedding_matches()\n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            A final list of entities that represents the best\n",
        "            entity linking for the entire context\n",
        "        \"\"\"\n",
        "        top_k = 1\n",
        "        top_strings = {}\n",
        "        for dictionary in list_of_dicts:\n",
        "            for key, value in dictionary.items():\n",
        "                if key in top_strings:\n",
        "                    top_strings[key].extend([t[0] for t in value[:top_k]])\n",
        "                else:\n",
        "                    top_strings[key] = [t[0] for t in value[:top_k]]\n",
        "        merged_list = list(set([s for sublist in top_strings.values() for s in sublist]))\n",
        "        return merged_list\n",
        "    \n",
        "    def print_log(self, sent_set, arg1_lst, named_entities, faiss_dict):\n",
        "        \"\"\"\n",
        "        Tracking method for determining possible algorithmic improvments.\n",
        "        This shows where each extracted query comes from (SRL or NER), the \n",
        "        quality of matches for each method, and --, all grouped on a per-sentnece\n",
        "        basis for each context\n",
        "        Parameters:\n",
        "        -----------\n",
        "        sent_set : list[Str]\n",
        "            List of string sentences from context\n",
        "        arg1_lst : list[Str]\n",
        "            List of extracted string queries from SRL model, for each sentence\n",
        "        named_entities : list[Str]\n",
        "            List of extracted string queries from NER model, for each sentence \n",
        "        faiss_dict : List[Dict(Tuple)]\n",
        "            List of consolidated top_k candiates for each query of the context sentence\n",
        "        Returns:\n",
        "        --------\n",
        "        log.txt\n",
        "            Text file containing all metrics.\n",
        "        \"\"\"\n",
        "        with open('log.txt', 'w') as f:\n",
        "            for i in range(len(sent_set)):\n",
        "                f.write(\"Sentence: \" + sent_set[i] + \"\\n\")\n",
        "                f.write(\"SRL Match: \" + str(arg1_lst[i]) + \"\\n\")\n",
        "                f.write(\"Entity Extract Match: \" + str(named_entities[i]) + \"\\n\")\n",
        "                f.write(\"FAISS Results: \" + \"\\n\")\n",
        "                for k, v in faiss_dict[i].items():\n",
        "                    f.write(\"QUERY: \" + k + \"\\n\")\n",
        "                    f.write(\"Ranked Results\" + \"\\n\")\n",
        "                    for ele in v:\n",
        "                        f.write(str(ele) + \"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "    \n",
        "    \n",
        "    def print_model_log(self, file_path = \"test.jsonl\"):\n",
        "        \"\"\"\n",
        "        Model Tracking of which algorithms and/or modifications were used, \n",
        "        and their resulting accuracy metrics.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        Returns:\n",
        "        --------\n",
        "        model_alter.txt\n",
        "            Text file containing all model metrics with performance.\n",
        "        \"\"\"\n",
        "        \n",
        "        with open('model_alter.txt', 'w') as f:\n",
        "            mean_accuracy, median_accuracy, \\\n",
        "            lowest_accuracy, highest_accuracy, context_count = self.get_accuracy_metrics(file_path)\n",
        "            f.write(\"Model Summary\\n\"\n",
        "                    f\"Input Nodes Dataset: {self.datafile}\\n\"\n",
        "                    f\"Input Pre-Embedding File: {self.embeddings_data}\\n\"\n",
        "                    f\"FAISS Tokenizer: {self.tokenizer}\\n\"\n",
        "                    f\"{'*'*20} Query Extractors {'*'*20}\\n\"\n",
        "                    f\"SRL Model: {self.srl_model_link}\\n\"\n",
        "                    f\"NER Model: MedSpacy + Regex\\n\"\n",
        "                    f\"{'*'*20} Node Candidate Extractors {'*'*20}\\n\"\n",
        "                    f\"String Matching: NONE\\n\"\n",
        "                    f\"Vector Search: FAISS\\n\"\n",
        "                    f\"Top K Nodes per Sentence: 1\"\n",
        "                    f\"{'*'*20} Final Accuracy Results {'*'*20}\\n\"\n",
        "                    f\"Number of Contexts: {context_count}\\n\"\n",
        "                    f\"Average Accuracy: {mean_accuracy}\\n\"\n",
        "                    f\"Median Accuracy: {median_accuracy}\\n\"\n",
        "                    f\"Lowest Accuracy: {lowest_accuracy}\\n\"\n",
        "                    f\"Highest Accuracy: {highest_accuracy}\\n\")\n",
        "\n",
        "            \n",
        "\n",
        "    def get_accuracy_metrics(self, file_path = \"test.jsonl\"):\n",
        "        \"\"\"\n",
        "        Parses through output file and obtains accuracies metrics for\n",
        "        all contextes in the file.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        file_path : .jsonl file\n",
        "            The file containing all the contexts after have been parsed with entity linking\n",
        "        Returns:\n",
        "        --------\n",
        "        [mean_accuracy, median_accuracy, lowest_accuracy, highest_accuracy] floats\n",
        "            Specific accuracy metrics, as specified by variable name.\n",
        "        \"\"\"\n",
        "        with open(file_path) as f:\n",
        "            accuracies = []\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                accuracies.append(data[\"accuracy\"])\n",
        "\n",
        "            mean_accuracy = np.mean(accuracies)\n",
        "            median_accuracy = np.median(accuracies)\n",
        "            lowest_accuracy = min(accuracies)\n",
        "            highest_accuracy = max(accuracies)\n",
        "\n",
        "            return mean_accuracy, median_accuracy, lowest_accuracy, highest_accuracy, len(accuracies)\n",
        "#######################################################################\n",
        "\n",
        "    def predict(self,text, show_log = True):\n",
        "        \"\"\"\n",
        "        The mother of all functions. This is the main\n",
        "        function used to parse a context sentence.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : String\n",
        "            The entire string context\n",
        "        show_log : boolean, optional (default=True)\n",
        "            Whehter or not to create the log.txt file for tracking and debugging\n",
        "        Returns:\n",
        "        --------\n",
        "        list[Str]\n",
        "            The final list of entity linking used to represent the entire context\n",
        "        \"\"\"\n",
        "        sent_set = self.extract_sentences(text)\n",
        "\n",
        "        arg1_lst = []\n",
        "        named_entities = []\n",
        "\n",
        "        for sent in sent_set:\n",
        "            srl_output = self.srl_extractor(sent)\n",
        "            arg1_lst.append(srl_output)\n",
        "\n",
        "\n",
        "        #named_entities = list(map(lambda sent: named_entity_extractor(sent,medspacy_model), sent_set))\n",
        "        for sent in sent_set:\n",
        "            named_entities.append(self.named_entity_extractor(sent))\n",
        "\n",
        "        query_lst = self.merge_lists(arg1_lst, named_entities)            \n",
        "        faiss_dict = self.get_embedding_matches(query_lst)\n",
        "        \n",
        "        # remove duplicate faiss keys\n",
        "        faiss_dict = remove_duplicate_keys(faiss_dict)\n",
        "        \n",
        "        #remove default nodes\n",
        "        default_nodes = ['all', 'sad', 'old age', 'elderly',\n",
        "                        'abt-493', 'surgery complication',\n",
        "                        'fits','symptoms and signs']\n",
        "        \n",
        "        faiss_dict = [{k:v for k,v in d.items() if v not in default_nodes} for d in faiss_dict]\n",
        "        \n",
        "        \n",
        "        #get linked entities\n",
        "        linked_entities = get_unique_values(faiss_dict)\n",
        "        \n",
        "        \n",
        "\n",
        "#         if show_log:\n",
        "#             self.print_log() \n",
        "\n",
        "        #return final list of input entities\n",
        "        #for each sentence, get the top 1 FAISS RESULTS\n",
        "\n",
        "        return faiss_dict,linked_entities\n",
        "\n",
        "\n",
        "    def save_output(self, input_data=\"data/medqa-ddb-handmap.jsonl\", output_data=\"test.txt\"):\n",
        "        \"\"\"\n",
        "        Similar to self.run_all, only instead of manually inputting the \n",
        "        context as the argument, the user can instead pass a file containing\n",
        "        many contexts to parse.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        input_data : .jsonl file\n",
        "            The dataset containing all the contexts over which we wish to parse\n",
        "        output_data : .txt file\n",
        "            A copy of the input_data, but with the added fields after processing.\n",
        "            Namely: \"linked_auto\" (the final entity linking list) and \"accuracy\" \n",
        "            (how well \"linked_auto\" matches to \"linked_entities\" field)\n",
        "        Returns:\n",
        "        --------\n",
        "        .txt file\n",
        "            A modified version of the input_data containing the parsing fields\n",
        "        \"\"\"\n",
        "        # Open the JSONL input file and create a new output file\n",
        "        with jsonlines.open(input_data) as reader, open(output_data, mode='w') as writer:\n",
        "\n",
        "            # Loop through all records in the input file\n",
        "            for record in reader:\n",
        "\n",
        "                # Apply the run_all function to the question\n",
        "                faiss_dict,linked_auto = self.predict((record['question']))\n",
        "\n",
        "                # Add the linked_auto list to the record as a new field\n",
        "                record['linked_auto'] = linked_auto\n",
        "                record['query_node_mapping'] = faiss_dict\n",
        "\n",
        "                # Compare the linked_entities and linked_auto lists and calculate accuracy\n",
        "                linked_entities = record['linked_entities']\n",
        "                num_correct = len(set([i.lower() for i in linked_entities]).intersection(set([i.lower() for i in linked_auto])))\n",
        "                accuracy = (num_correct / len(linked_entities))\n",
        "\n",
        "                # Add the accuracy to the record as a new field\n",
        "                record['accuracy'] = accuracy\n",
        "\n",
        "                # Write the updated record to the output file\n",
        "                writer.write(\"question\\n\")\n",
        "                writer.write(record['question'] + \"\\n\")\n",
        "                writer.write(\"linked_entities\\n\")\n",
        "                writer.write(str(record['linked_entities']) + \"\\n\")\n",
        "                writer.write(\"linked_auto\\n\")\n",
        "                writer.write(str(record['linked_auto']) + \"\\n\")\n",
        "                writer.write(\"query_node_mapping\\n\")\n",
        "                writer.write(str(record['query_node_mapping']) + \"\\n\")\n",
        "                writer.write(\"accuracy\\n\")\n",
        "                writer.write(str(record['accuracy']) + \"\\n\\n\")\n",
        "\n",
        "                \n",
        "def remove_duplicate_keys(dicts_list):\n",
        "    \"\"\"\n",
        "    Removes duplicate keys in a list of dictionaries.\n",
        "    Args:\n",
        "    - dicts_list: a list of dictionaries\n",
        "    Returns:\n",
        "    - a list of dictionaries with duplicate keys removed\n",
        "    \"\"\"\n",
        "\n",
        "    output_list = []\n",
        "    keys_set = set()\n",
        "\n",
        "    for d in dicts_list:\n",
        "        new_dict = {}\n",
        "        for k, v in d.items():\n",
        "            if k not in keys_set:\n",
        "                new_dict[k] = v\n",
        "                keys_set.add(k)\n",
        "        output_list.append(new_dict)\n",
        "\n",
        "    return output_list\n",
        "\n",
        "def get_unique_values(data):\n",
        "    unique_values = set()\n",
        "    for d in data:\n",
        "        for v in d.values():\n",
        "            unique_values.add(v)\n",
        "    return list(unique_values)\n",
        "\n",
        "def parse_sentence(sentence):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(sentence)\n",
        "    output = []\n",
        "    \n",
        "    if len(doc) == 3 and doc[1].text == 'and':\n",
        "        output.append(doc[0].text)\n",
        "        output.append(doc[2].text)\n",
        "        return output\n",
        "    else:\n",
        "        return sentence\n",
        "    \n",
        "    for i, token in enumerate(doc):\n",
        "        if token.pos_ == 'ADJ' and doc[i+1].pos_ == 'NOUN':\n",
        "            if doc[i+2].text == 'and':\n",
        "                output.append(token.text + ' ' + doc[i+1].text)\n",
        "            elif doc[i+2].pos_ == 'ADP' and doc[i+3].pos_ == 'ADJ' and doc[i+4].pos_ == 'NOUN':\n",
        "                output.append(token.text + ' ' + doc[i+1].text + ' ' + doc[i+3].text + ' ' + doc[i+4].text)\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "kF52FMxJJd4D"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall spacy -y\n",
        "!pip install spacy==3.4.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfxEMF9qWVv6",
        "outputId": "b9a681af-d773-4a7d-d32a-3cb6b2578ce4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: spacy 3.4.4\n",
            "Uninstalling spacy-3.4.4:\n",
            "  Successfully uninstalled spacy-3.4.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==3.4.3\n",
            "  Using cached spacy-3.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (4.65.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.1.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (8.1.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (0.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (2.28.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (1.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (1.24.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (67.6.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.4.3) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy>=0.3.5->spacy==3.4.3) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.4.3) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.3) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.3) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.3) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.3) (3.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.3) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.3) (0.7.9)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy==3.4.3) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy==3.4.3) (2.1.2)\n",
            "Installing collected packages: spacy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.0.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.4.3 which is incompatible.\n",
            "allennlp 2.1.0 requires spacy<3.1,>=2.1.0, but you have spacy 3.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linker = entity_linker(embeddings_data=\"ddb_embeddings_synonym.pickle\",\n",
        "                 datafile = \"ddb_nodes_header.csv\")"
      ],
      "metadata": {
        "id": "HW4SnU2_J-vd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linker.run_pre_embedd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611,
          "referenced_widgets": [
            "55e4c845fcec4066a3d361604cb2fa69",
            "e16d5ae2dd58439cade6f7165212a44c",
            "b3ec09df91454003911fb4adfee89011",
            "b7364b4d63604c3fa240c36039b8f921",
            "6c13eab24815485b8ce948aea42d5f1f",
            "2fac9bc916dc4b74ae165d51d2b6e5b2",
            "a8909628e940449ebd785a95b28d263b",
            "1ab1da5bf2534fc7b6bfdcfa6a5097f5",
            "b5322f32f9dc4f89a4a710e7b8e3027f",
            "d77eb4c3dd90473a844be666ae28df16",
            "7053fd65085542bdba021d9c1342ed3a"
          ]
        },
        "id": "lTFtCejtS-Se",
        "outputId": "995c1eb1-ac92-4cc0-d771-9c322e6544c4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting pre-embedding\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/17022 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55e4c845fcec4066a3d361604cb2fa69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings done\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a014723dbce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pre_embedd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-d8d1a198ed4f>\u001b[0m in \u001b[0;36mrun_pre_embedd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embeddings done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0membeddings_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_faiss_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"faiss added\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36madd_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[1;32m   4752\u001b[0m         \"\"\"\n\u001b[1;32m   4753\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatted_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4754\u001b[0;31m             super().add_faiss_index(\n\u001b[0m\u001b[1;32m   4755\u001b[0m                 \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4756\u001b[0m                 \u001b[0mindex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/search.py\u001b[0m in \u001b[0;36madd_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         faiss_index = FaissIndex(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/search.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, device, string_factory, metric_type, custom_index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaiss_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_has_faiss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;34m\"You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;34m\"A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmrsly0ZXE3J",
        "outputId": "ec4384f5-05d1-4de0-f71a-9316fe402571"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.9/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss"
      ],
      "metadata": {
        "id": "XfZ4aPoWY9ir"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip fr"
      ],
      "metadata": {
        "id": "4pIi1pnjaAS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}